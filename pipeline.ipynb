{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46a8e9ce-5abf-40f2-b0e2-9847b60725be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kfp 1.8.14 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\n",
      "google-cloud-pipeline-components 1.0.25 requires google-cloud-storage<2,>=1.20.0, but you have google-cloud-storage 2.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn) (1.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.6)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\n",
      "Installing collected packages: sklearn\n",
      "Successfully installed sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q\n",
    "! pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bea67b3-89c9-45bc-aa47-69279bf7150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9a1e235-82cc-4a4d-ab44-ae846b5b3b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 1.8.14\n"
     ]
    }
   ],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae212af7-1d53-408b-85c3-7034d7531fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deae2edf-9d5a-410d-8159-808f597908cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"de2022-362611\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-west1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"gs://jads_temp_de2022_michimalek\"\n",
    "MODEL_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77bf4973-c5d0-4d96-ba4d-784fa8737b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=None, squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "324a0026-9d96-4860-aef2-ad50fcfcd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_comp = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff04f550-2f98-416e-bf65-9b41606fb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "def train_lr (features: Dict, project_id: str, model_repo: str) -> Dict:\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "        \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)  \n",
    "    \n",
    "    logging.info(f\"Cols: {df.columns}\")        \n",
    "    \n",
    "    x_train, x_test, y_train, y_test = train_test_split(df.drop(['target',\"Unnamed: 0\"],axis=1), \n",
    "                                                        df['target'], test_size=0.30, \n",
    "                                                        random_state=101)\n",
    "\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": model.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/local_model.pkl'\n",
    "    joblib.dump(model, local_file)\n",
    "    # write out output\n",
    "  \n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('lrmodel.pkl')\n",
    "   # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "  \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e442919-1531-4719-b813-7e089e4ef648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for training lr model\n",
    "trail_lr_com = kfp.components.create_component_from_func(\n",
    "    train_lr, output_component_file='train_lr_model.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4ba45a4-52b3-4a32-9200-381cd1549b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lr(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)    \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob('lrmodel.pkl')\n",
    "    filename = '/tmp/local_model.pkl'\n",
    "    blob.download_to_filename(filename)\n",
    "        \n",
    "    #Loading the saved model with joblib\n",
    "    model = joblib.load(filename)\n",
    "    \n",
    "    logging.info(df.columns)\n",
    "\n",
    "\n",
    "    xNew = df[[\"sepal length (cm)\",\"sepal width (cm)\",\"petal length (cm)\",\"petal width (cm)\"]]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pclass'] = y_classes.tolist()\n",
    "    dic = dfcp.to_dict(orient='records') \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3380051d-3fc2-4f85-91a1-85c09ae310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for prediction LR \n",
    "prediction_lr_com = kfp.components.create_component_from_func(\n",
    "    predict_lr, output_component_file='prediction_lr_com.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3386249-51b7-4208-bb70-7ca2060d12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"iris-predictor-training-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str, ):\n",
    "    \n",
    "    \n",
    "    di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "     \n",
    "    training_lr_job_run_op = trail_lr_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=di_op.output\n",
    "    )\n",
    "    \n",
    "    pre_di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_lr_job_run_op)\n",
    "\n",
    "    predict_lr_job_run_op = prediction_lr_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=pre_di_op.output\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4eebf5b6-316d-4c89-8e8c-e5a950015451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a86b49c1-d119-4a8e-b4ca-51fc851b3b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/iris-predictor-training-pipeline-20221023163513?project=26340798564\n",
      "PipelineJob projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/26340798564/locations/us-central1/pipelineJobs/iris-predictor-training-pipeline-20221023163513\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"iris-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"iris_predictor_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, # makesure to use your project id \n",
    "        'data_bucket': 'data_de2022_michimalek',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'full_data.csv',     # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'testset_filename': 'test.csv',    # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'model_repo': 'model_de2022_michimalek' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc45cdd-7506-4bd7-8b77-01ab8885fb9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Local)",
   "language": "python",
   "name": "local-base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
