{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a8e9ce-5abf-40f2-b0e2-9847b60725be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# The Vertex AI Workbench Notebook product has specific requirements\n",
    "IS_WORKBENCH_NOTEBOOK = os.getenv(\"DL_ANACONDA_HOME\") and not os.getenv(\"VIRTUAL_ENV\")\n",
    "IS_USER_MANAGED_WORKBENCH_NOTEBOOK = os.path.exists(\n",
    "    \"/opt/deeplearning/metadata/env_version\"\n",
    ")\n",
    "\n",
    "# Vertex AI Notebook requires dependencies to be installed with '--user'\n",
    "USER_FLAG = \"\"\n",
    "if IS_WORKBENCH_NOTEBOOK:\n",
    "    USER_FLAG = \"--user\"\n",
    "\n",
    "! pip3 install --upgrade google-cloud-aiplatform {USER_FLAG} -q\n",
    "! pip3 install -U google-cloud-storage {USER_FLAG} -q\n",
    "! pip3 install {USER_FLAG} kfp google-cloud-pipeline-components --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bea67b3-89c9-45bc-aa47-69279bf7150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a1e235-82cc-4a4d-ab44-ae846b5b3b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae212af7-1d53-408b-85c3-7034d7531fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import component\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deae2edf-9d5a-410d-8159-808f597908cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Google Cloud project that this pipeline runs in.\n",
    "project_id = \"project-id\"\n",
    "# The region that this pipeline runs in\n",
    "region = \"us-west1\"\n",
    "# Specify a Cloud Storage URI that your pipelines service account can access. The artifacts of your pipeline runs are stored within the pipeline root.\n",
    "pipeline_root_path = \"pipeline_root_path\"\n",
    "MODEL_PATH = \"model.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf4973-c5d0-4d96-ba4d-784fa8737b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "def download_data(project_id: str, bucket: str, file_name: str) -> Dict:\n",
    "    '''download data'''\n",
    "    from google.cloud import storage\n",
    "    from sklearn.datasets import load_iris\n",
    "    import pandas as pd\n",
    "    import logging \n",
    "    import sys\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    # Downloaing the file from a google bucket \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(bucket)\n",
    "    blob = bucket.blob(file_name)\n",
    "    local_path = '/tmp/'+ file_name\n",
    "    blob.download_to_filename(local_path)\n",
    "    logging.info('Downloaded Data!')\n",
    "    \n",
    "    # Convert the data to a dictiory object    \n",
    "    dict_from_csv = pd.read_csv(local_path, index_col=None, squeeze=True).to_dict()\n",
    "    logging.info('Returning Data as Dictionary Object!')\n",
    "    return dict_from_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a0026-9d96-4860-aef2-ad50fcfcd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for data ingestion\n",
    "data_ingestion_comp = kfp.components.create_component_from_func(\n",
    "    download_data, output_component_file='data_ingestion.yaml', packages_to_install=['google-cloud-storage', 'pandas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04f550-2f98-416e-bf65-9b41606fb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Dict\n",
    "def train_lr (features: Dict, project_id: str, model_repo: str) -> Dict:\n",
    "    '''train a LogisticRegression with default parameters'''\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn import metrics\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging \n",
    "    import sys\n",
    "    import os\n",
    "    import joblib\n",
    "        \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)  \n",
    "    \n",
    "    logging.info(df.columns)        \n",
    "    \n",
    "    model = LogisticRegression()\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": model.score(x_test, y_test)\n",
    "    }\n",
    "    logging.info(metrics)\n",
    "   \n",
    "    # Save the model localy\n",
    "    local_file = '/tmp/' + MODEL_PATH\n",
    "    joblib.dump(model, local_file)\n",
    "    # write out output\n",
    "  \n",
    "    # Save to GCS as model.h5\n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "   # Upload the locally saved model\n",
    "    blob.upload_from_filename(local_file)\n",
    "  \n",
    "    print(\"Saved the model to GCP bucket : \" + model_repo)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e442919-1531-4719-b813-7e089e4ef648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for training lr model\n",
    "trail_lr_com = kfp.components.create_component_from_func(\n",
    "    train_lr, output_component_file='train_lr_model.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ba45a4-52b3-4a32-9200-381cd1549b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lr(project_id: str, model_repo: str, features: Dict) -> Dict:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from google.cloud import storage\n",
    "    import json\n",
    "    import logging\n",
    "    import sys\n",
    "    import os\n",
    "    \n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(features)    \n",
    "    \n",
    "    client = storage.Client(project=project_id)\n",
    "    bucket = client.get_bucket(model_repo)\n",
    "    blob = bucket.blob(MODEL_PATH)\n",
    "    filename = '/tmp/' + MODEL_PATH\n",
    "    blob.download_to_filename(filename)\n",
    "        \n",
    "    #Loading the saved model with joblib\n",
    "    model = joblib.load(filename)\n",
    "\n",
    "    xNew = df[['ntp', 'age', 'bmi', 'dbp', 'dpf', 'pgc', 'si', 'tsft']]\n",
    "\n",
    "    dfcp = df.copy()   \n",
    "    y_classes = model.predict(xNew)\n",
    "    logging.info(y_classes)\n",
    "    dfcp['pclass'] = y_classes.tolist()\n",
    "    dic = dfcp.to_dict(orient='records') \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3380051d-3fc2-4f85-91a1-85c09ae310cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a KFP component for prediction LR \n",
    "prediction_lr_com = kfp.components.create_component_from_func(\n",
    "    predict_lr, output_component_file='prediction_lr_com.yaml', packages_to_install=['google-cloud-storage', 'pandas', 'joblib', 'scikit-learn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3386249-51b7-4208-bb70-7ca2060d12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the workflow of the pipeline.\n",
    "@kfp.dsl.pipeline(\n",
    "    name=\"diabetes-prdictor-training-pipeline\",\n",
    "    pipeline_root=pipeline_root_path)\n",
    "def pipeline(project_id: str, data_bucket: str, trainset_filename: str, model_repo: str, testset_filename: str, ):\n",
    "    \n",
    "    \n",
    "    di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=trainset_filename\n",
    "    )\n",
    "     \n",
    "    training_lr_job_run_op = trail_lr_com(\n",
    "        project_id=project_id,\n",
    "        model_repo=model_repo,       \n",
    "        features=di_op.output\n",
    "    )\n",
    "    \n",
    "    pre_di_op = data_ingestion_comp(\n",
    "        project_id=project_id,\n",
    "        bucket=data_bucket,\n",
    "        file_name=testset_filename\n",
    "    ).after(training_mlp_job_run_op, training_lr_job_run_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eebf5b6-316d-4c89-8e8c-e5a950015451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import compiler\n",
    "compiler.Compiler().compile(pipeline_func=pipeline,\n",
    "        package_path='iris_predictor_training_pipeline.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86b49c1-d119-4a8e-b4ca-51fc851b3b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "job = aip.PipelineJob(\n",
    "    display_name=\"iris-predictor\",\n",
    "    enable_caching=False,\n",
    "    template_path=\"iris_predictor_training_pipeline.json\",\n",
    "    pipeline_root=pipeline_root_path,\n",
    "    parameter_values={\n",
    "        'project_id': project_id, # makesure to use your project id \n",
    "        'data_bucket': '',  # makesure to use your data bucket name \n",
    "        'trainset_filename': 'training_set.csv',     # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'testset_filename': 'prediction_set.csv',    # makesure to upload these to your data bucket from DE2022/lab4/data\n",
    "        'model_repo': '' # makesure to use your model bucket name \n",
    "    }\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.14 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "a665b5d41d17b532ea9890333293a1b812fa0b73c9c25c950b3cedf1bebd0438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
